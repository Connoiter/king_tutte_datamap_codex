{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# codex__datamap_20_newsgroups.ipynb\n",
        "<div>\n",
        "<img src=\"https://connoiter.com/images/tutte/king_tutte_on_colab.transparent_bg.png\" align=\"left\" width=\"200px\"/>\n",
        "\n",
        "This notebook, [codex__datamap_20_newsgroups.ipynb](https://github.com/Connoiter/king_tutte_datamap_codex/blob/main/by_repo/codex/codex__datamap_20_newsgroups.ipynb), is part of [the King Tutte Datamap Codex repo](https://github.com/Connoiter/king_tutte_datamap_codex). The Codex is a Jupyter Book consisting of Jupyter notebooks that run on Colab. They run King Tutte datamap pipelines on Google's Colab.\n",
        "</div>\n",
        "<br/>\n",
        "\n",
        "## Pedigree\n",
        "\n",
        "This notebook was started from scratch for the Codex project. The Tononymy code was taken from [Getting Started with Toponymy](https://toponymy.readthedocs.io/en/latest/basic_usage.html).\n",
        "\n",
        "## What this notebook does\n",
        "\n",
        "This notebook\n",
        "1. Set-up: Pip downloads a Gemma model from Hugging Face to be run on the GPU on Colab. Gemma is used as the topic labeler for Toponymy\n",
        "2. Downloads and reads a Parquet file from Hugging Face, [a subset of the 20 Newsgroups](https://huggingface.co/datasets/lmcinnes/20newsgroups_embedded) dataset (18.2k rows)\n",
        "3. Samples of topic clusters are run through Toponomy to generate short labels for clusters/topics\n",
        "4. DataMapPlot generates the datamap webapp (newsgroups.html)\n",
        "\n"
      ],
      "metadata": {
        "id": "eF8bITq0ahQ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set-up"
      ],
      "metadata": {
        "id": "4gHmOPUoXY4S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Start stopwatch"
      ],
      "metadata": {
        "id": "R7MhnobbYsu4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Start a timer. Record wall clock time now and again\n",
        "# later to determine total runtime of notebook\n",
        "import time\n",
        "from datetime import timedelta\n",
        "\n",
        "start = time.time()\n"
      ],
      "metadata": {
        "id": "84bAtG8eiEWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset\n",
        "\n",
        "The dataset this notebook works on is a subset of the Twenty Newsgroups dataset. This subset consists of 18,xxx documents from the newsgroups dataset.\n",
        "\n",
        "On free tier Colab with a T4 GPU, it takes over an hour to datamap all ~18k rows. So, to speed things up for demo purposes, the 18k are chopped down to 1k. Feel free to not do that."
      ],
      "metadata": {
        "id": "LGkLp1UsCZFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "PA3PZy8KjLlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "newsgroups_df = pd.read_parquet(\"hf://datasets/lmcinnes/20newsgroups_embedded/data/train-00000-of-00001.parquet\")\n",
        "newsgroups_df = newsgroups_df.head(1000)\n"
      ],
      "metadata": {
        "id": "MjaiYErfixYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Various pip installs"
      ],
      "metadata": {
        "id": "SCL9uX4xZbNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade transformers accelerate bitsandbytes torch huggingface-hub\n",
        "!pip install -q toponymy\n",
        "!pip install -q datamapplot"
      ],
      "metadata": {
        "id": "D8cjvjargaoS",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Auth set-up\n",
        "\n",
        "Hugging Face auth is required for Gemma download, and user has to visit model card on huggingface.co to click OK on ELUA."
      ],
      "metadata": {
        "id": "p9xSkRNVYzfP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: why this way and not just secret => os.environ[]\n",
        "#!hf auth login"
      ],
      "metadata": {
        "id": "HkwvPHIaK2EQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00a29416"
      },
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Retrieve the Hugging Face token from Colab Secrets\n",
        "# Make sure you have a secret named 'HF_TOKEN' containing your token\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "\n",
        "if hf_token:\n",
        "    print(\"This notebook has found the existing HF_TOKEN secret\")\n",
        "    # Set the token as an environment variable\n",
        "    os.environ['HF_TOKEN'] = hf_token\n",
        "    print(\"Hugging Face token loaded from secrets and set as environment variable, HF_TOKEN.\")\n",
        "else:\n",
        "    print(\"HF_TOKEN secret is not set, or access to it has not been granted.\")\n",
        "    !hf auth login\n",
        "    # hf auth login should set HF_TOKEN environment variable for transformers to find while auth'ing model download\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bitsandbytes is for 8-bit CUDA, already pip'd above\n",
        "#!pip install --upgrade --quiet bitsandbytes"
      ],
      "metadata": {
        "id": "vlV9GVTSggv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO:JFT Not sure why this isn't a code cell. This code is recommending usings 4-bit quant on Colab's T4, should do that, which is not the case in later cells.\n",
        "\n",
        "```python\n",
        "# Load Model and Tokenizer with Quantization\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "# Model ID for the efficient 2B-equivalent Gemma 3n Instruct model\n",
        "# Note: You must have accepted the license on the Hugging Face model card for this to work.\n",
        "model_id = \"google/gemma-3n-E2B-it\"\n",
        "\n",
        "# Configuration for 4-bit quantization (recommended for T4 GPU)\n",
        "# TODO:JFT, yeah, T4. Do this elsewhere, say, with gemma:1B\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",  # Normal Floats 4-bit\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16, # Compute in BFloat16 for better precision\n",
        "    bnb_4bit_use_double_quant=True, # Optional: double quantization for more memory savings\n",
        ")\n",
        "\n",
        "# Load the Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# Load the Model\n",
        "# device_map=\"auto\" automatically manages memory and places the model on the GPU.\n",
        "# The model will use the HF_TOKEN stored in Colab Secrets for authentication.\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "\n",
        "print(f\"Model {model_id} loaded successfully in 4-bit mode!\")\n",
        "```"
      ],
      "metadata": {
        "id": "W2g7qbpUjZP5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Gemma model from Hugging Face\n",
        "\n",
        "Toponymy needs to be configured with an LLM to summarized document descriptions.\n",
        "\n",
        "- [x] Take #1: `google/gemma-3n-E2B-it`, ran out of memory after over 2h on a T4\n",
        "- [ ] Take #2: `google/gemma-3-1b-it`, ~2GB, worked took almost 2h\n",
        "- [ ] Take #3: `google/gemma-3n-E2B-it`\n",
        "\n"
      ],
      "metadata": {
        "id": "6w6GJvBCygjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from toponymy.llm_wrappers import HuggingFaceNamer\n",
        "\n",
        "# Initialize with a Hugging Face model\n",
        "# llm will be used by Toponymy as llm_wrapper=llm\n",
        "llm = HuggingFaceNamer(\n",
        "    model=\"google/gemma-3-1b-it\", # out of memory: \"google/gemma-3n-E2B-it\", OutOfMemoryError: CUDA out of memory.\n",
        "    llm_specific_instructions=\"Generate clear, descriptive topic names\",\n",
        "    device_map=\"auto\",  # Automatically map to available devices\n",
        "    dtype=torch.bfloat16  # Use bfloat16 precision for efficiency and better precision\n",
        ")"
      ],
      "metadata": {
        "id": "pqStVE0Kc1Qo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Toponymy basics\n",
        "\n",
        "Following example code from: [Getting Started with Toponymy](https://toponymy.readthedocs.io/en/latest/basic_usage.html)."
      ],
      "metadata": {
        "id": "iS3-MWQRiwSc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# newsgroups_df.head()\n",
        "# = index,post,newsgroup,embedding,map\n",
        "# is int,str,str,long-vec,2-vec\n",
        "display(newsgroups_df)"
      ],
      "metadata": {
        "id": "Tlro3iU4jnHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if True:\n",
        "    # TODO: next up, make embeddings and run UMAP. Never did that; started with embeds & umaps already computed\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    from umap import UMAP\n",
        "\n",
        "    embedding_model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
        "    embedding_vectors = embedding_model.encode(newsgroups_df[\"post\"], show_progress_bar=True)\n",
        "    clusterable_vectors = UMAP(metric=\"cosine\").fit_transform(embedding_vectors)\n",
        "else:\n",
        "    # the embeddings and UMAP (x,y) coord already exist, no need to compute those:\n",
        "    embedding_vectors = np.stack(newsgroups_df[\"embedding\"].values)\n",
        "    clusterable_vectors = np.stack(newsgroups_df[\"map\"].values)"
      ],
      "metadata": {
        "id": "yL7JojlAj9Q4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from toponymy import Toponymy, ToponymyClusterer, KeyphraseBuilder\n",
        "from toponymy.llm_wrappers import AzureAINamer\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "embedding_model = SentenceTransformer(\"paraphrase-MiniLM-L3-v2\")\n",
        "\n",
        "#azure_api_key = open(\"../azure_cohere_api_key.txt\").read().strip()\n"
      ],
      "metadata": {
        "id": "FYzzE0O4kfyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model = Toponymy(\n",
        "    llm_wrapper=llm,\n",
        "    text_embedding_model=embedding_model,\n",
        "    clusterer=ToponymyClusterer(min_clusters=4, verbose=True),\n",
        "    keyphrase_builder=KeyphraseBuilder(ngram_range=(1,6), max_features=15_000, verbose=True),\n",
        "    object_description=\"newsgroup posts\",\n",
        "    corpus_description=\"20-newsgroups dataset\",\n",
        "    exemplar_delimiters=[\"<EXAMPLE_POST>\\n\",\"\\n</EXAMPLE_POST>\\n\\n\"],\n",
        ")"
      ],
      "metadata": {
        "id": "s9BBkqwTktes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "topic_model.fit(\n",
        "    newsgroups_df[\"post\"].str.strip().values,\n",
        "    embedding_vectors=embedding_vectors,\n",
        "    # NO show_progress_bar=topic_model.show_progress_bars, # topic_model.clusterer.fit()\n",
        "    clusterable_vectors=clusterable_vectors\n",
        ")"
      ],
      "metadata": {
        "id": "11qSRZ6Kk-Wb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "elapsed = time.time() - start\n",
        "print(f\"Total runtime: {timedelta(seconds=elapsed)}\")"
      ],
      "metadata": {
        "id": "yRwqlaUd2F_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.topic_names_[-1]"
      ],
      "metadata": {
        "id": "0jVOZQanPVkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "newsgroups_df.newsgroup.unique().tolist()"
      ],
      "metadata": {
        "id": "OapWNVAEPaDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Topic treeview\n",
        "\n",
        "Below is the `topic_tree_`, a treeview of the topics as labeled by Toponymy. The nodes in the tree can expand and collapse to show child topics."
      ],
      "metadata": {
        "id": "r0OP8AP-BP-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.topic_tree_"
      ],
      "metadata": {
        "id": "zOdvozLbPVcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataMapPlot"
      ],
      "metadata": {
        "id": "5pOuOu7zPj2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datamapplot\n",
        "import datamapplot.selection_handlers"
      ],
      "metadata": {
        "id": "5OcOHjF9Pme2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot = datamapplot.create_interactive_plot(\n",
        "    clusterable_vectors,\n",
        "    *topic_model.topic_name_vectors_,\n",
        "    title=\"20-Newsgroups\",\n",
        "    sub_title=\"A data map of 20-newsgroups using all-mpnet-basev2, Toponymy, Cohere and UMAP\",\n",
        "    hover_text=newsgroups_df[\"post\"].values,\n",
        "    font_family=\"Cormorant SC\",\n",
        "    marker_size_array=np.asarray([np.log(len(x)) for x in newsgroups_df[\"post\"].values]),\n",
        "    colormaps={\"newsgroup\": pd.Series(newsgroups_df[\"newsgroup\"].values)},\n",
        "    cluster_layer_colormaps=True,\n",
        "    enable_search=True,\n",
        "    selection_handler=datamapplot.selection_handlers.WordCloud(height=300),\n",
        ")\n",
        "plot"
      ],
      "metadata": {
        "id": "zXbPI2gjP9SL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "plot.save('newsgroups.html')\n",
        "files.download('newsgroups.html')"
      ],
      "metadata": {
        "id": "FDTbUuXuabDc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}